# åç«¯é›†æˆæŒ‡å—

## ğŸ“– æ¦‚è¿°

InferUnityé‡‡ç”¨**åç«¯é›†æˆæ¶æ„**ï¼Œå°†æˆç†Ÿçš„æ¨ç†æ¡†æ¶ï¼ˆå¦‚ONNX Runtimeï¼‰ä½œä¸ºæ‰§è¡Œæä¾›è€…ï¼Œè€Œéé‡å¤å®ç°åº•å±‚ç®—å­ã€‚

## ğŸ—ï¸ æ¶æ„

```
ç”¨æˆ·ä»£ç 
    â†“
InferUnity API (ç»Ÿä¸€æ¥å£)
    â†“
ExecutionProvider Interface (åç«¯æŠ½è±¡å±‚)
    â†“
Backend Implementations
    â”œâ”€ ONNX Runtime (æ¨è)
    â”œâ”€ CPU (è¿‡æ¸¡å®ç°)
    â””â”€ å…¶ä»–åç«¯ (å¯é€‰)
```

## ğŸ”§ ä½¿ç”¨æ–¹å¼

### 1. ä½¿ç”¨ONNX Runtimeåç«¯ï¼ˆæ¨èï¼‰

```cpp
#include "inferunity/engine.h"

using namespace inferunity;

int main() {
    // åˆ›å»ºä¼šè¯é€‰é¡¹
    SessionOptions options;
    options.execution_providers = {"ONNXRuntime"};  // ä½¿ç”¨ONNX Runtimeåç«¯
    options.graph_optimization_level = SessionOptions::GraphOptimizationLevel::ALL;
    
    // åˆ›å»ºæ¨ç†ä¼šè¯
    auto session = InferenceSession::Create(options);
    if (!session) {
        std::cerr << "Failed to create session" << std::endl;
        return 1;
    }
    
    // åŠ è½½æ¨¡å‹
    Status status = session->LoadModel("model.onnx");
    if (!status.IsOk()) {
        std::cerr << "Failed to load model: " << status.GetMessage() << std::endl;
        return 1;
    }
    
    // å‡†å¤‡è¾“å…¥
    std::vector<Tensor*> inputs = {input_tensor};
    std::vector<Tensor*> outputs = {output_tensor};
    
    // æ‰§è¡Œæ¨ç†
    status = session->Run(inputs, outputs);
    if (!status.IsOk()) {
        std::cerr << "Inference failed: " << status.GetMessage() << std::endl;
        return 1;
    }
    
    return 0;
}
```

### 2. è‡ªåŠ¨é€‰æ‹©åç«¯

```cpp
SessionOptions options;
// ä¸æŒ‡å®šexecution_providersï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
options.execution_providers = {};  // æˆ–çœç•¥

auto session = InferenceSession::Create(options);
// ç³»ç»Ÿä¼šæŒ‰ä¼˜å…ˆçº§é€‰æ‹©ï¼šONNX Runtime > CPU
```

### 3. æŸ¥è¯¢åç«¯èƒ½åŠ›

```cpp
auto& registry = ExecutionProviderRegistry::Instance();
auto providers = registry.GetRegisteredProviders();

for (const auto& name : providers) {
    auto provider = registry.Create(name);
    if (provider && provider->IsAvailable()) {
        std::cout << "Backend: " << provider->GetName() << std::endl;
        std::cout << "Version: " << provider->GetVersion() << std::endl;
        std::cout << "Device: " << (int)provider->GetDeviceType() << std::endl;
    }
}
```

## ğŸ› ï¸ ç¼–è¯‘é…ç½®

### å¯ç”¨ONNX Runtimeåç«¯

```bash
# å®‰è£…ONNX Runtimeï¼ˆmacOSï¼‰
brew install onnxruntime

# æˆ–ä»æºç ç¼–è¯‘
git clone https://github.com/microsoft/onnxruntime.git
cd onnxruntime
./build.sh --config Release --build_shared_lib

# é…ç½®CMake
cmake .. -DENABLE_ONNXRUNTIME=ON \
         -Donnxruntime_DIR=/path/to/onnxruntime/build

# ç¼–è¯‘
make -j$(nproc)
```

### ä»…ä½¿ç”¨CPUåç«¯ï¼ˆè¿‡æ¸¡å®ç°ï¼‰

```bash
# ä¸å¯ç”¨ONNX Runtime
cmake .. -DENABLE_ONNXRUNTIME=OFF
make -j$(nproc)
```

## ğŸ“Š åç«¯å¯¹æ¯”

| ç‰¹æ€§ | ONNX Runtime | CPU (å†…éƒ¨å®ç°) |
|------|-------------|---------------|
| æ€§èƒ½ | â­â­â­â­â­ ä¼˜ç§€ | â­â­ åŸºç¡€ |
| ç¨³å®šæ€§ | â­â­â­â­â­ ç»è¿‡éªŒè¯ | â­â­â­ æµ‹è¯•ä¸­ |
| ç®—å­æ”¯æŒ | â­â­â­â­â­ å®Œæ•´ONNXæ ‡å‡† | â­â­ éƒ¨åˆ†ç®—å­ |
| SIMDä¼˜åŒ– | â­â­â­â­â­ æ·±åº¦ä¼˜åŒ– | âŒ æœªå®ç° |
| GPUæ”¯æŒ | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ |
| æ¨èåœºæ™¯ | ç”Ÿäº§ç¯å¢ƒ | æµ‹è¯•/å¼€å‘ |

## ğŸ¯ æœ€ä½³å®è·µ

1. **ç”Ÿäº§ç¯å¢ƒ**ï¼šä½¿ç”¨ONNX Runtimeåç«¯
2. **å¼€å‘æµ‹è¯•**ï¼šå¯ä»¥ä½¿ç”¨CPUåç«¯ï¼ˆå¦‚æœONNX Runtimeä¸å¯ç”¨ï¼‰
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆ©ç”¨ONNX Runtimeçš„å›¾ä¼˜åŒ–ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰
4. **å¤šåç«¯**ï¼šæœªæ¥å¯ä»¥åŒæ—¶æ”¯æŒå¤šä¸ªåç«¯ï¼Œæ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **ONNX Runtimeä¾èµ–**ï¼šéœ€è¦å•ç‹¬å®‰è£…ONNX Runtimeåº“
2. **æ¨¡å‹æ ¼å¼**ï¼šåç«¯æœŸæœ›ONNXæ ¼å¼æ¨¡å‹
3. **å†…å­˜ç®¡ç†**ï¼šTensorå†…å­˜ç”±InferUnityç®¡ç†ï¼Œåç«¯è´Ÿè´£æ‰§è¡Œ
4. **é”™è¯¯å¤„ç†**ï¼šåç«¯é”™è¯¯ä¼šé€šè¿‡Statusè¿”å›

---

**ä¸‹ä¸€æ­¥**ï¼šå®ç°ONNX Runtimeåç«¯çš„å®Œæ•´é›†æˆå’Œæµ‹è¯•

